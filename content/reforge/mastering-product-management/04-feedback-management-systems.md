Introduction to Feedback Management Systems

In this module, we’re going to be talking about feedback management systems.

To set the stage for what these systems are and why they’re so important, let’s dive into the story of Facebook and one of its major decisions for its messaging product, Messenger.

In 2014, Facebook made a seemingly strange, but ultimately correct, product decision.

They decided to fully separate out their messaging feature into a separate app called Facebook Messenger.

There was no obvious reason for Facebook to separate out Messenger.

In fact, there seemed to be obvious reasons for Facebook to not do so.

For example, users would be required to download a completely separate Messenger app in order to chat with each other. This would create a lot of friction for users who had grown accustomed to doing their messaging natively in the Facebook app.

Additionally, pulling Messenger out was a complex and burdensome execution effort, requiring dedicated, focused resources.

So how did the Facebook team get the conviction to make such a drastic product change?

The answer is they had strong product intuition that they needed to.

Product intuition is the instinct you develop as a PM to make great product decisions, even if you don’t have a full set of information to guide you.

In Facebook’s case, they had steadily been observing that Facebook messaging was facing stiff competition from competing products, showing up in a suboptimal user growth rate.

Additionally, they also noticed user expectations had been shifting to expect a more robust messaging experience within the app.

If they didn’t separate out Messenger, they were at risk of being disrupted by the ongoing unbundling trend in the market.

As time has shown, they made the correct choice.

Separating Messenger into its own app ultimately allowed Facebook to reach a much larger audience base. In the 3 years after the initial decision, Facebook Messenger’s Monthly Active Users grew 6.5x, from 200 million to 1.5 billion.

A lot of net new users seemed to not care about Facebook’s social networking features, and primarily just needed a way to communicate.

Great PMs use their product intuition to help them make great product decisions in the absence of detailed information. They can identify what will work for their end users without needing to do a whole battery of research studies, thus effectively creating a type of leverage for themselves.

Most people think product intuition is an innate skill. However, in reality, product intuition is a developed skill.

Sachin Rekhi says, “You develop product intuition by constantly exposing yourself to what customers are saying. Over the course of thousands of data points, you start to instinctively understand what will work and what won’t.”

In other words, you develop product intuition by: constantly immersing yourself in reading customer feedback, pattern-matching across all those data points, and synthesizing those patterns into insights.

We call this process of using feedback to develop product intuition feedback immersion.

Feedback immersion refers to the act of submerging yourself in user context with feedback, and then synthesizing insights from it.

Similar to how you immerse yourself in a foreign culture to better learn the language, immersing yourself in user feedback enables you to better understand what will resonate with the user.

Feedback can be:

Explicit customer feedback: where users say directly what they want. For example, in NPS verbatim, users may explicitly ask for product changes like new features, improvements to buggy experiences, or other important changes.

Implicit customer feedback: where you infer user wants and needs from their actions. For example, help search queries give you implicit clues on what users find unintuitive about your product.

In Facebook’s case, they had several data points of customer feedback:

Explicit feedback included:

Requests for pure messaging vs. social networking features.

User requests for Facebook to match the better experiences of other messaging apps.

An example of implicit feedback was:

Observing users being slow to view messages due to extra clicks to access them.

Through regular exposure to these user requests and behaviors, they built up the intuition that they needed to make a drastic change with Messenger.

However, there are three major challenges PMs face when they try to proactively develop product intuition:

Friction in establishing a regular feedback immersion habit.

Challenges in maintaining a direct understanding of the user.

and overweighing the importance of recent feedback in decision-making.

The first challenge PMs face is friction.

This happens for two reasons. The first is a 'pull’ relationship with feedback.

Most PMs have a “pull” type relationship with their feedback, meaning they have to remember to check for feedback and intentionally “pull” it from a system.

As a result, most PMs end up looking at their feedback sources fairly irregularly.

The next challenge is friction in accessing feedback. After they’ve decided to go look at feedback, they have to manually go into the system that holds the feedback data and create a query for it, doing this for each source of feedback they want to look at.

If NPS data lives in a software like Wootrics, for example, you have to get access from the marketing team.

If churn survey data lives in Google Sheets, you have to figure out where it’s stored on the drive and then format it into a readable document.

As a result of these points of friction, the PM ends up looking at feedback on an infrequent basis, and looking at a non-holistic set of data sources when they do look at feedback -- both of which ultimately hinder their ability to build product intuition.

The second challenge most PMs face is difficulty in maintaining a direct understanding of the user.

Sachin Rekhi says “As an organization gets bigger, I always find it takes far more effort to actually talk to your customers.”

In other words, as organizations scale, a common problem PMs run into is they have increasingly less direct, firsthand exposure to the voice of their users, because it feels like they have to talk to so many more people. Instead, they start to rely on secondhand exposure to understand their users.

For example, instead of talking directly to their users, they might rely on reading user research reports done by a user research team.

The problem with secondhand research is that you start to lose the original Voice of Customer (VOC).

This results in a less nuanced understanding of who your users are and what they actually want.

The secondhand study might say, “users indicated auto-scrolling is a high priority, citing competitor parity.“

Contrast it to how a user might describe it. The true user commentary might be, “Every other reading software out there has auto-scroll. This is a basic need for making reading more seamless. I’m quitting your product until you build it in.”

The secondhand study direct is far weaker than the direct VOC in conveying the user’s context, their wants, and the intensity of their need.

The third challenge most PMs face is that they tend to over-index recent feedback.

Instead of having an objective view on what’s most requested, PMs tend to over-weigh the most recent feedback in their decision making

This tendency to over-weigh recent feedback is known as recency bias.

The problem is that this results in incorrect product intuition for what users actually want, leading to suboptimal product outcomes.

For example, consider a hypothetical case at Bumble.

Bumble is a social dating app, where people can upload photos of themselves and be matched to other users based on their mutual interest.

Let’s say Bumble had several requests for major matching algorithm improvements, all of which would significantly improve the user experience. Then, let’s assume one of its competitors, Tinder, launched video profile pictures, where users can upload short videos of themselves instead of static photos.

Bumble might suddenly be inundated with feature requests to support video profile pictures as well.

Without a system, it would be easy for Bumble to prioritize video for profile photos over matchmaking improvements. Video for profile photos might be the most recently requested product change, but a matchmaking algorithm improvement is the most requested overall.

The problem with doing so in this hypothetical scenario is that the % of users that care about video profiles seems large in perception, but is actually much smaller.

As a result, prioritizing video profiles would make the small percentage of the user base on the cutting edge of profile optimization happy, but not move the needle at all for the huge remaining audience of the Bumble population.

Clearly, PMs face some major challenges in being able to effectively develop their product intuition.

So, how do great PMs overcome these challenges and develop their product intuition then?

The secret is that great PMs use feedback management systems.

Feedback management systems automate the collection and processing of feedback from multiple sources.

They centralize all feedback in a single system, removing the friction that usually blocks PMs from regularly looking at multiple sources of feedback. Additionally, the systems preserve the original voice of the customer so that PMs can keep getting a direct understanding of their user at scale.

They also quantitatively track total requests for features, allowing PMs to sense whether they are falling victim to recency bias.

For PMs who have a broader scope than just specific feature work, these systems also extend to tracking broader product requests.

For example, a PM at Wise might want to track requests for new markets to serve.

Or similarly, a PM at Reforge might want to track requests for new programs.

These aren’t necessarily feature requests, but can be tracked all the same in the feedback management systems we talk about.

In this module, and for simplicity in language, we’ll talk about tracking feature requests; know that these systems can also extend to broader product change request tracking.

When used together, feedback management systems allow great PMs to better develop their product intuition, creating leverage for them.

In other words, they are able to make better product decisions with less dedicated research.

In this module, we'll talk about the feedback management systems great PMs use to develop and maintain product intuition.

We'll introduce two critical feedback systems: the feedback river and the feedback system of record.

A feedback river is a system that consolidates and integrates your multiple feedback sources into a single channel.

The feedback river provides you with a regularly-updating stream of user data points from multiple feedback sources with which to hone your product intuition.

In lesson one, we’ll explain what exactly feedback rivers are and how they build your product intuition.

Then, in lesson two, we’ll walk through how to set yours up.

In lesson three, we’ll cover feedback systems of record.

A feedback system of record is a system that quantitatively tracks how many users have requested a feature over time. It helps prevent you from falling victim to recency bias, and helps you track what users most want, enabling you to better calibrate your product intuition.

We’ll explain what exactly feedback systems of record are, and how to set yours up.

And finally, in lesson four, we’ll show you to use these systems in tandem to develop your product intuition.

Let’s jump in.

Feedback River

In our module introduction, we discussed how great PMs create leverage by using their product intuition to make great product decisions in the absence of detailed information.

However, there are major challenges most PMs run into that hold them back from effectively developing product intuition. The major challenges are: friction in establishing a regular feedback immersion habit, challenges in maintaining a direct understanding of the user, and overweighing the importance of recent feedback in their decision-making.

To solve these challenges, we talked about how great PMs setup feedback management systems; these systems automate the collection and processing of feedback from multiple sources.

In this lesson, we’ll cover the first of those tools: feedback rivers. We’ll explain what they are and how they help you build product intuition.

First let's talk about what a feedback river is.

A feedback river is a system that consolidates your multiple feedback sources into a single channel. The channel is usually Slack, or an email mailing list.

For an example of a Slack-based feedback river, consider Wise.

Feedback from their various feedback sources gets pushed into two separate channels, #happy-customer and #unhappy-customer, based on the user sentiment.

In other words, NPS survey responses from promoters are sent to #happy-customer, and responses from detractors are sent to #unhappy-customer. Similarly, customer support publishes happy user feedback to #happy-customer, and unhappy feedback to #unhappy-customer.

PMs skim the channels to get data points on what customers like and dislike, helping them develop their product intuition.

For a mailing list-based river, consider LinkedIn Sales Navigator.

Feedback from their various feedback sources is sent via email into a mailing list named lighthouse-feedback@linkedin.com(opens in a new tab). Monthly NPS survey results are sent to the inbox, with major feedback themes summarized and supported with verbatim quotes. They similarly piped in emails from the rest of their feedback sources, including:

Questions from the sales team.

Reasons for lost deals from Salesforce.

Churn survey responses, among others.

Everyone has access to this inbox, and PMs regularly skim through to see what the latest customer sentiment is, helping them develop their product intuition.

Now let's talk about how feedback rivers build product intuition.

The feedback river addresses the first major problem PMs have in developing product intuition: friction in establishing good feedback immersion habits.

The feedback river removes both major friction points: "pull" relationship with feedback, and friction in accessing feedback

First, feedback rivers solve the "pull" relationship with feedback problem by changing the relationship to more of a "push."

Recall that most PMs have a pull type relationship with their feedback, meaning they have to remember to check for feedback and intentionally pull it from a system.

As a result, most PMs end up looking at their feedback sources fairly irregularly, which causes their feedback cycle time to length.

Feedback cycle time is the time it takes between releasing a feature and reading user feedback on how it was received.

When you have longer feedback cycle times, it becomes harder to intuit what users like and are looking for.

The feedback river solves this problem by changing the traditional pull dynamic of feedback into more of a "push" dynamic.

In other words, as feedback on a certain feature comes in, it automatically gets sent (or “pushed”) to the feedback river, where the PM notices there is fresh feedback to read through.

This 'push' dynamic removes the need for the PM to remember to look at feedback, and the steady flow of new feedback creates a regular trigger for the PM to go read it.

To illustrate how powerful this difference is, let’s consider a Slack example. First, imagine if a PM who’s been launching features at Slack didn’t have a convenient way to access feedback.

After their initial feature launch, they might not remember to consciously go check on user feedback very often.

As a result, they lose a lot of data points of what users like and dislike. Contrast this if the PM at Slack was regularly checking their feedback river. They might not remember to specifically look for feedback on their launched features.

However, because the feedback river is constantly streaming in user feedback, they have a high chance of casually colliding into some feedback that reminds them about what they’ve worked on.

Second, feedback rivers help reduce friction in accessing feedback. Most PMs experience friction in accessing feedback from a variety of data sources.

As an example, when a PM wants to access feedback from a new system, the PM has to:

Get access to the system data.

Learn how to pull the system data.

Properly structure a query each time they want to pull the data.

Format the query output into a more readable format.

These moments of added friction result in busy PMs looking only at a limited set of feedback sources.

However, looking at a limited set of feedback sources is a problem, because each feedback source offers a unique perspective on the user.

By looking at a narrow set of feedback sources, PMs miss a holistic view of the user.

For example, help search queries and customer support tickets uniquely help you understand what people find unintuitive about your product.

However, because pulling useful data from these sources of feedback tends to be highly manual, most PMs don’t look at them.

As a result, they miss key opportunities to make their product design more intuitive.

To illustrate this, consider an example at Pinterest.

Pinterest is a social media site focused on image sharing, saving, and organization.

Before they did a redesign, Pinterest’s name for saving images was called ‘Pinning’ an image. In other words, the button to save an image was labeled ‘Pin’ instead of ‘Save’.

This wasn’t particularly intuitive to users and as a result many users didn’t save images.

This was problematic because Pinterest’s algorithm generated engaging content based on the user’s saved pins. When users didn’t save pins, they were far less likely to be engaged.

Had Pinterest been looking at help search queries, they could’ve spotted this problem early on. They might’ve seen lots of queries on ‘how to save an image’ or ‘what is pinning’.

Based on that finding, Pinterest could have determined they needed to rename the ‘Pin’ action to a more intuitive ‘Save’ action.

Feedback rivers solve this feedback collection friction problem by centralizing all feedback into one easily-accessible channel.

Instead of needing to go into a tool like Google Sheets to dig up NPS survey results and a tool like Asana for customer tickets, data from both automatically feeds into the Slack channel which they are already logged into.

As a result, they only need to skim the feedback river to see the latest data pushes from both sources, and avoid having to log in and query each source.

Additionally, feedback rivers also help PMs develop their product intuition by facilitating a more direct understanding of the user. In order to handle the scale, they start to rely on secondhand research. However, in doing so, they lose the direct Voice of Customer meaning they lose a lot of detail on the user’s context, their wants, and the intensity of their needs.

Feedback rivers solve this secondhand exposure problem by preserving user verbatims from the feedback sources. User verbatims are the exact comments users put in their original feedback submissions.

The feedback river directly passes the user verbatims along with each feedback input, giving the PM more first hand exposure to the user and their needs.

This firsthand exposure in turn enables the PM to better understand the severity and context of the user’s needs, helping them develop their product intuition.

So to recap, feedback rivers help you remove the friction in developing a regular feedback immersion habit and get a more direct understanding of the user.

In the next lesson, we'll cover how to set up your feedback river.

Setting Up Your Feedback River

In the previous lesson, we talked about feedback rivers, and how they develop product intuition.

We covered how feedback rivers consolidate feedback from different sources into a single channel.

And we talked about how feedback rivers help PMs better develop their product intuition by:

Removing friction involved in establishing good feedback habits, and

Facilitating a more direct understanding of the user

But how do you set up your feedback river?

There are three key steps:

Determine feedback river platform

Integrate programmatic feedback sources

Integrate manual feedback sources

We’ve listed these steps for following along in the Reforge Feedback River Template.

The first step is to determine your feedback river platform.

The most common platforms used are Slack and an email mailing list, but most internal communication tools work perfectly fine as well.

The most important consideration in choosing between platforms is which platform has the lowest adoption barrier. In other words, how integrated the platform is with people’s existing workflows.

It’s important to optimize for, because you ideally want everyone on your team to be regularly reading through the feedback river and developing their sense of the user.

The lower the adoption barrier, the more likely it is that people skim through the feedback river from time to time.

Since everyone at Wise used Slack for communication already, Slack was the logical choice to use for their feedback river.

In contrast, since LinkedIn wasn’t a Slack-native culture, they set up an email mailing list instead.

The second step is to integrate your programmatic feedback sources into the river.

Programmatic feedback sources are feedback sources for which you can automatically initiate feedback collection and publish to the feedback river.

Some standard feedback sources here might include:

NPS surveys

In-product feedback submissions

Churn surveys

Outside of these standard sources, think also about other feedback sources that give you user insight, but might not be those you typically use. Some examples would be:

Help search queries: what people search for in your help database.

Social media mentions: the product buzz you get on social media networks like Facebook, Twitter, LinkedIn, and Instagram.

A longer list of common programmatic feedback sources is tracked on the Reforge Feedback River Template.(opens in a new tab)

The major aha moment in this step is realizing the value of looking at different feedback sources.

In this step, PMs that typically only look at a few sources of feedback are looking at a much larger variety of feedback sources, and find they get a much more holistic view of user sentiment around their product.

For example, consider the unique perspectives that some of the example sources listed above provide:

NPS offers you a unique lens on what users love, giving you a sense of what to double-down on.

These responses offer a great contrast to most feedback sources, which focus on the negative parts of your product that people want improved.

Help Search Queries show you what users find unintuitive, helping you to improve your product design.

Social media mentions reveal how your users describe your product to others, giving you an idea of what use cases your product best serves and how it’s perceived.

Consider an example at Notejoy.

They had been getting several user requests in their feature request tracker for developing an Evernote importer, a tool to help users migrate their existing notes from Evernote to Notejoy.

However, they had not prioritized it due to the relatively low request volume.

What they didn’t realize was that most of their core users had already committed to manually migrating notes or abandoning their archive.

As a result, they of course didn’t see much request volume on their feature tracker for note migration support.

However, although only a few users requested note importer features, it was still a meaningful problem that was silently pulling Notejoy’s activation rate down.

They realized this problem when they implemented their feedback river.

Due to more regular use of the feedback river, the Notejoy team started reading through cancellation surveys more regularly.

They noticed "lack of note migration tool" as an increasingly common cancellation reason.

This led to a timely decision to prioritize implementing the Evernote importer, opening up a large new user acquisition opportunity.

Summing up what we’ve covered so far, it’s important to get a holistic picture of the user from multiple sources in order to build your product intuition.

Integrating your programmatic feedback sources is a great first step.

But there are challenges to doing a good integration. The biggest of these challenges for most PMs is handling programmatic inputs at scale.

They start to get too many inputs from certain feedback sources, which leads to other feedback sources never being read.

For example, if they get a large volume of help search queries relative to inputs from all other feedback sources, the help search query messages end up comprising a disproportionate amount of the feedback river.

This results in PMs largely seeing only help search query messages in the feedback river, and losing their holistic picture of what other feedback sources are saying about their product.

This then weakens their ability to build product intuition.

To handle these scaling issues, there are three different types of scaling approaches you can take, depending on the data source:

Sampling

Selection

Summarization

The first approach you can take is sampling. Sampling refers to when you publish every nth input instead of every single input.

Sampling is most applicable to high-volume surveys.

These would be your NPS surveys and your churn surveys.

The “n” you choose is referred to as your sampling interval.

To find what sampling interval to use, start small and adjust it upwards until you feel like you’re getting a holistic set of inputs again from your feedback river.

For example, let’s say that you’re getting so many NPS survey responses that you rarely see inputs from the other feedback sources anymore.

If you publish every 2nd NPS survey, and it still feels like you’re not getting a holistic picture, adjust it to every 3rd NPS survey instead. If still too little, continue adjusting upwards, until you feel like you’re getting to regularly see other feedback sources again.

The second approach you can take is selection.

Selection refers to when you force the input to meet a set of "significance criteria" before being published.

Selection is most often used to moderate help search query volume.

For example, you might require that you get at least 3 or more queries on the same search term before it’s deemed significant enough to push to the feedback river.

This helps you isolate one-off problems from more systematic usability issues users are having.

It can also be used to moderate social media mention volume.

You might require that a post get a certain number of likes, retweets, shares, or views before publishing it to the feedback river.

This helps you find the highest-reach social media posts that impact how others perceive and talk about your product.

The third approach you can take is summarization.

Summarization refers to when you rely on a team to distill a large volume of inputs into its core takeaway points.

For example, for NPS, you might rely on the marketing team to read through a whole month’s NPS responses. They would write up the key takeaways in a monthly summary and publish it to the feedback river.

Between these options, it’s usually preferable to try using sampling or selection before going to summarization.

Summarization is less preferable than sampling and selection for 2 reasons:

Feedback cycle slow-down

and Dilution of VOC.

The first challenge with summarization is that it tends to slow down your feedback cycle.

Summarization requires someone to collect a critical mass of inputs, synthesize it, and then finally publish it.

These activities all add feedback cycle time, reducing the speed at which you can learn about user reactions to your product.

The second challenge with summarization is that you tend to dilute the original voice of the customer, or VOC.

Regardless of how well they’re trained to preserve original VOC, the process of summarization naturally tends to result in some bias from the synthesizer.

This means you risk losing some of the nuance you’d get from reading feedback inputs directly.

As a result, we recommend using summarization only when you are starting to miss key user insights due to a high sampling interval.

To navigate the downsides of summarization, you can do two things:

Faster summarization times, or

More user verbatims

The first thing you can do is aim for more regular summarization cycles.

For example, instead of having quarterly NPS summaries, ask for monthly NPS summaries instead. These shorter cycles enable you to get closer to a more real-time view of user sentiment.

The second thing you can do is ask for as many verbatims as possible in the summaries.

For example, let’s say a key theme in an NPS summary is “users interested in multi-user, real-time collaboration.”

The summarizer should support this theme with the original comments that relate to it.

For example, they’d list quotes like, “It’d be great to have multiple people be able to work on the document at the same time, similar to Google Docs.”

Preserving the original voice of the customer via verbatims helps you stay more grounded to what the user is truly feeling.

So in summary, in step two, you:

Integrate your programmatic feedback sources, and

Scale them with a sampling, selection, or summarization approach.

This step alone is already a great start to enriching the quantity and variety of feedback sources you use to develop product intuition.

The third step is to integrate your manual feedback sources into the river.

Manual feedback sources are feedback sources for which you can’t programmatically collect and publish inputs. Rather, you have to ask the responsible team to do some level of synthesis and then publish to the feedback river.

This would include sources like:

Support tickets: Summaries of the most important themes coming out of customer support tickets. A designated lead publishes the top issues, with representative verbatim quotes.

Research interviews: Summaries of completed user interviews. The research lead publishes the key takeaways, with representative verbatim quotes, along with a link to the original raw interview notes.

Metrics investigations: Sometimes, metrics move in an unexpected way. Strong product teams look into why these metrics have moved in order to refine their understanding of metric drivers. The product team publishes the metric movement, along with the hypothesized drivers for the metric movement.

For B2B businesses, this also usually includes:

Sales + customer success feedback: Summaries of key customer wins & stories, as well as top customer concerns.

Sales loss data: Top reasons for lost deals. The top reason usually tends to be "price too high," but there will often be product-related reasons too that are more actionable.

The primary challenge PMs run into in this step is managing stakeholder input.

The challenge is that in order for teams to keep on contributing feedback, they need to feel heard.

However, it can be very time-consuming to help disparate individuals really feel heard.

This is especially a challenge for managing inputs from sales teams.

For example, if you’re getting lots of feedback from individual sales reps, you often won’t be able to action all of their feedback.

Explaining your feature prioritization decisions to each one of them will help them better feel heard, but that takes a lot of time.

This creates a dilemma: you want your stakeholders to give you valuable user feedback, but can’t individually address all of their concerns.

To solve this problem, designate product champions.

A product champion is a chosen representative for a given team.

All feedback from the team funnels through them, and they effectively act as the product's ambassador to answer questions from the team.

Having a single point of contact for each team makes feedback collection and handling much easier, because you now only deal with one person instead of a whole team.

For example, LinkedIn chose to have 1 product champion from sales and 1 from customer support in order to better manage stakeholder input.

Similarly, consider Asana, the project management software company.

Asana’s user support, customer success, and sales teams each have 1 designated manager who synthesizes the feedback from across the team.

They’re responsible for consolidating all input and prioritizing it into a list of top-10 issues for the product to address.

They also are the most directly connected to the product team, and serve as the main contact for understanding why specific feedback might’ve been prioritized over others.

When selecting your product champion, there are three key traits you want them to have:

Context-oriented, i.e. will explain the ‘why’ behind specific feature asks. This enables you to better build your product intuition.

Product-empathetic, i.e. can appreciate that there are trade-offs behind decisions. This prevents you from getting into a situation where the representative simply demands features without understanding how those features will impact the rest of the product.

Technically capable, i.e. has a basic grasp of the technologies you use. This makes communicating much easier.

Once this is done and you’ve integrated your manual feedback sources, you now have a fully running feedback river.

You now have the ability to frictionlessly look at a large quantity of feedback inputs from a broad variety of feedback sources, giving you the raw data with which to hone your product intuition.

In the next lesson, we’ll cover another type of feedback management system, the feedback system of record.

The feedback system of record solves the second major problem in developing product intuition: overweighting recency of feedback when evaluating feedback importance.

Feedback Systems of Record

In our module introduction, we talked about how great PMs create leverage by using their product intuition to make great product decisions in the absence of detailed information.

However, there are two major challenges most PMs run into that hold them back from effectively developing product intuition. They are:

Friction in building good feedback immersion habits, and

Over-weighing recency of feedback when evaluating feedback importance.

To solve these challenges, we talked about how great PMs set up feedback management systems: systems that automate the collection and processing of feedback from multiple sources.

In the previous lesson, we covered the first of those systems: feedback rivers.

Feedback rivers solve the first two challenges:

Friction in establishing a regular feedback immersion habit

Challenges in maintaining a direct understanding of the user

In this lesson, we’ll introduce a tool to solve the third challenge.

This tool is called the feedback system of record.

A feedback system of record (FSOR) is a system for quantitatively tracking your feature requests over time.

At its most basic, here’s how an FSOR works.

As feedback comes in with specific feature requests, those feature requests are added to the FSOR.

If the feature request already exists, the number of votes for that particular feature is increased.

The FSOR differs from the feedback river in two key ways:

Structural tracking

Less qualitative detail

First, the FSOR structurally tracks how often a feature has been requested.

When feedback comes in, if it has a feature request, the FSOR updates itself on how often that feature has been requested.

In contrast, the feedback river does not keep a running count of how often a feature has been asked for.

Second, the FSOR stores less qualitative detail.

When a feature request comes in, the FSOR only updates itself on how often it’s been requested.

In other words, the FSOR does not store the original user verbatim and context behind the feature request.

The feedback river stores the full original user verbatim, allowing the PM to better understand the context behind a feature request.

As a result of these differences, the feedback system of record is much more uniquely suited to help PMs solve their third challenge in developing product intuition: overweighting recent feedback.

Recall that recency bias is the tendency to over-weigh recent feedback.

In other words, PMs may erroneously think a feature is requested more often than it actually is, just because it was requested more recently and is fresher in their mind.

The problem is that the PMs perceived importance of a feature doesn’t match the objective importance for it, which leads to suboptimal feature prioritization decisions.

The FSOR helps the PM avoid this trap by being an objective record of how often features really are requested.

To illustrate this, recall the hypothetical Bumble example we talked about earlier.

Without a FSOR, Bumble PMs might not have realized the requests for video profile pictures aren’t objectively as requested as matchmaking improvements, and made a sub-optimal prioritization choice.

So, we know that FSOR are important for mitigating recency bias . Next, let's talk about how to set up a FSOR

Setting up your FSOR is a 3-step process:

Determine your FSOR platform

Set up your FSOR

Regularly update your FSOR with feedback input

Let’s walk through this 3-step process in sequence. The third step tends to be the most complex, so we’ll spend the most time there when we get to it.

Your first step is to determine what platform you want to use for your SOR.

Any software that allows you to track data in a structured way will generally work as an FSOR.

Some common platforms used are Google Sheets, Trello, and Jira.

If your users are particularly engaged, you can also consider using a public-facing feedback software like Canny, which allows users to directly upvote and submit feature requests.

We recommend choosing your platform based on what you feel will be easiest to update for your organization, because keeping the FSOR up to date is one of the most challenging parts of maintaining a useful FSOR.

If most of your feature requests are submitted through customer support, for instance, then you may want to make the FSOR the same as the system that customer support uses.

At LinkedIn, spreadsheets ended up being the simplest platform for their main sources of feature requests (customer support and sales) to update, so they ran with that.

For Notejoy, they decided to use a public-facing feedback tracker for their FSOR, because their users were highly active and regularly submitted feature requests directly.

Your second step is to set up your FSOR with the fields you want to track.

The key fields that a FSOR needs to track are:

The feature request

Short description of what it is

Number of requests received

Current status of the feature request - resolved, in progress, de-prioritized, or open

For B2B companies, you also will usually need to track the following fields:

Type of user requesting the feature - end user or business decision-maker. End users are the people who use the software day to day. Business decision-makers are the people who don’t use the software day to day, but hold the buying power for the software.

Current spend from the requesting users’ business

Potential spend from the requesting users’ business

It’s useful to track these fields for B2B, because you often need to weigh feedback differently depending on who’s asking for a feature.

For example, it’s often most important to satisfy the business decision-maker’s requirements before the end users, because they hold the buying power.

Similarly, if you have a large customer or potential customer, you likely will need to prioritize their feedback over other smaller customers.

A shell FSOR template is provided in the Reforge Feedback System of Record Template(opens in a new tab).

Finally, in the most difficult part of the process, set up a regular updating process for the FSOR.

Most PMs struggle with this step, because they don’t have a good system to manage all the disparate sources of input they get.

Feedback requests often come from multiple sources, in unstructured data formats.

Manually reading through data and then updating the FSOR for each feedback input is extremely time-consuming.

As a result, the FSOR ends up not being updated, losing its value as a source of truth for how requested a feature really is.

To solve this problem, great PMs create a system where the teams responsible for each feedback source update the FSOR.

Since these teams already have someone assigned to reading through the feedback source, it’s much less of an incremental lift to parse out key feedback requests to update the FSOR with.

There are three major challenges to successfully leveraging other teams though:

Maintaining FSOR data cleanliness

Managing product champion capacity

Motivating other teams to tag the feedback into the hierarchy

The first challenge is maintaining the FSOR’s data cleanliness.

Teams that process the feedback sources won’t necessarily know the product as well as you do.

As a result, they might inadvertently create multiple feature requests, all termed slightly differently, for the same underlying feature request.

For example, they might create 3 different entries (mobile save, on the go note creation, browser 1-click save) for the same feature request (web clipper).

To solve this, have a single designated product champion from each team be responsible for updating the FSOR.

This will generally be the same product champion you chose to handle your manual feedback sources in the feedback river setup process.

Having one person in charge of updating the FSOR allows them to build up knowledge of what feature requests are being tracked in the FSOR.

This is beneficial, because they know what feature requests already exist in the FSOR and the alternative names that they often go by.

As a result, they are much more likely to avoid creating a duplicate new feature request.

Additionally, having one person in charge of the FSOR has an added efficiency benefit.

As a product champion repeatedly sees certain feature requests over and over, they can begin to more efficiently distinguish when a certain piece of feedback is related to an existing request.

If you’ve elected to have a public-facing FSORs where users can directly create and upvote feature requests, you will also need a community moderator.

At its most basic level, the community moderator is a designated person to maintain FSOR hygiene.

Users won’t always look for existing feature requests to upvote before creating a new one.

As a result, you need a community moderator to help with merging duplicate feature requests.

If the moderator has capacity, they should also maintain community relations. They should let users know that their feedback has been received and help communicate the relative priority level of certain feature requests.

This helps users feel like they’re being heard, which keeps them continually contributing.

At the start, you will likely need to designate a person from your organization to handle these tasks.

Over time, certain users will emerge as particularly passionate and knowledgeable about your product, at which point you can appoint them as moderators.

This practice is seen in Microsoft user forums, where there are both Microsoft-employed full-time moderators and passionate user moderators.

The second challenge is managing your product champion’s capacity.

Often, your product champion won’t be able to dedicate all their time to processing feedback into FSOR updates.

As a result, you have to efficiently prioritize the work that they can do.

This challenge is most applicable for product champions from customer support teams, as they tend to have the most feedback volume to read through.

Efficiently prioritizing work means making sure the feedback they spend time reading through and categorizing into feature requests are representative of the most requested features.

You can help them identify what features are most requested by creating a categorization system for feature requests.

A categorization system is a system for sorting objects into different areas based on their attributes.

For an example categorization system, consider the Dewey decimal system, used in most libraries.

It breaks up books into several major topic areas: from religion to science.

Then, within each topic area, it divides books up by sub-topic.

For example, within the social sciences topic area, it has sub-topics like statistics, economics, law, and several others.

Applying this idea to the FSOR, the idea is to ask teams to categorize feedback into broad feature areas as they’re processing it.

For example, as the customer support team responds to tickets, they label what feature area a specific feature request belongs to.

The product champion can then selectively read and process only the feedback from the most popular feature areas.

This prioritization methodology makes it far more likely that the feature requests they upvote in the FSOR are representative of the most requested ones.

For an example of this at work, consider what the LinkedIn Sales Navigator team did.

They had a huge volume of customer support tickets coming in. However, their product champion for customer support could only dedicate 1-2 hours per week to processing these tickets into FSOR upvotes.

To help the customer support product champion manage all this volume, they created a two-layer categorization system.

The first layer of labeling was for type of feedback. They categorized customer support tickets into 3 broad ticket types:

Feature Request

UX Issue

Bug

This first layer of categorization helped the product champion only look at relevant feature request tickets.

The second layer of categorization sorted each ticket into 14 distinct feature areas. These areas each represented a different part of the LinkedIn Sales Navigator product, from Feed to Profile to Accounts.

This allowed the product champion to see what feature areas had the most feature requests.

They prioritized feature areas with the highest number of feature requests in each review, making it more likely that the tickets they processed into feature requests for the FSOR were representative of the most requested features.

The final challenge is motivating other teams to participate in the FSOR updating process.

We recommend framing feedback processing for the FSOR as the primary channel through which other teams can influence product direction.

Sachin says, “Often, teams like customer support and sales are really interested in influencing the direction of the product. The key is to make sure that they feel heard and that their work is actually shaping the product.”

Consider how the product team at LinkedIn got buy-in from other teams to update the FSOR.

First, they framed the FSOR as the primary way for other teams to give feedback.

They noted that the product team looked at the FSOR on a bi-weekly basis to help calibrate what feature requests to prioritize, meaning the best way to get features prioritized was to show the quantitative request data for it on the FSOR.

Additionally, they invited cross-functional representatives from sales and customer support to attend those bi-weekly meetings to make sure that other teams’ feedback was being heard.

Representatives were required to update the FSOR with data from their respective feedback sources beforehand, creating a clear incentive for tagging.

In the meeting itself, the product team helped teams feel like they were being heard.

Representatives could be in the room while feature prioritization decisions were being made. For feature requests that were deprioritized, they could directly hear from and discuss the rationale with the PM team.

Finally, when announcing feature releases, the product team would credit the teams that updated that request in the FSOR, reinforcing the idea that the FSOR was the primary channel for input.

Together, these behavior reinforcement mechanisms helped keep the FSOR accurate and useful, giving the PM team a key tool to calibrate their product intuition.

That concludes setting up your FSOR.

You now have the knowledge to set up and run both key feedback management systems, enabling you to efficiently build your product intuition.

In the next lesson, we’ll cover the nuances involved in using these systems to develop your product intuition.

Using Feedback Management Systems

In our module introduction, we talked about how great PMs create leverage by using their product intuition to make great product decisions in the absence of detailed information.

However, there are two major challenges most PMs run into that hold them back from effectively developing product intuition.

The two major challenges are:

Friction in building good feedback-immersion habits

Overweighing recency of feedback when evaluating feedback importance

In the previous two lessons, we covered how great PMs create feedback rivers and feedback systems of record to get past these challenges.

We explained the 3-step process for setting up feedback rivers, as well as the 3-step process for setting up feedback systems of record.

In this lesson, we’ll go more deeply into how to use these systems.

There are two ways you can immediately put these systems to use: developing user insight and rapid feature research.

The primary way you use these feedback management systems is to develop your user insight.

Developing user insight can be split into two parts:

Developing initial hypotheses on what users want

Calibrating those hypotheses

Your feedback river handles the first part, while your feedback system of record helps with the second.

To develop your hypotheses on what users want, you have to regularly immerse yourself in the feedback river.

This means checking your feedback river daily for short periods of time to get a quick sense of the latest feedback themes, giving you the data points to start spotting patterns in user preferences.

It’s good to think of your feedback river like a Twitter feed.

You don’t have to read every single message in the feedback river when looking through it, but rather just skim through it to get a general sense of sentiment.

Sachin Rekhi even directly compares his feedback river habit to Twitter: “I treat the feedback river like Twitter. I just do a quick scroll while waiting in line or in otherwise unusable downtime.”

Regular, brief immersion tends to be better than intermittent, long periods of immersion.

Regular, brief immersion tends to be better for two reasons: easier habit formation, and less intuition atrophy.

The first reason regular, brief immersion is preferred is because it makes it easier to form good feedback immersion habits.

In other terms, consider a person who wants to exercise more. Someone who sets their bar for initiating exercise as a single pushup is much more likely to maintain good exercise habits than someone whose mindset is that they must exercise for an hour and a half each time they start.

The same analogy applies to a PM who wants to develop their product-intuition muscle.

A PM who views feedback immersion as a short scroll through the feedback river is much more likely to maintain their feedback immersion habits than a PM who thinks they have to sit down for an hour and a half each time.

The second reason regular, brief immersion is preferred is because it prevents intuition atrophy.

Intuition atrophy is when your product intuition weakens because you haven’t been keeping yourself updated on user feedback.

This is because new user feedback provides data points to calibrate your product intuition against. Without these new data points, you risk having an outdated understanding of what users truly want.

Sachin Rekhi says, “Building your product intuition is similar to strength training—you can't just do it all at once, you need to repeat it regularly to be effective.”

When immersing yourself in your feedback river, there are two specific tactics that help you develop your hypotheses faster: examining outliers, and context re-creation

The first tactic is examining outliers. When you get a particularly negative or positive piece of feedback, examine it further.

Understanding who loves your product and why will clue you in on your ideal target audience and how to build for them.

In contrast, understanding who hates your product and why will alert you to potential anti-audiences.

When examining feedback, we recommend looking at the feedback giver’s user profile and contributions to the feedback river.

Looking at their profile data helps you understand the demographics of your audience or anti-audience.

Looking at their contributions to the feedback river helps you understand your users’ context.

Seeing other problems they’ve complained about, or features they’ve particularly liked, will give you a better picture of what they really find frustrating or delightful.

For an example of how examining positive outliers was helpful in developing product hypotheses, consider a real example at Notejoy.

Over time, Notejoy was able to pattern match that the people who loved it the most valued Notejoy for its simplicity and speed. They also noticed that these highly positive users weren’t particularly tech-savvy, sometimes even explicitly saying so in their feedback commentary.

This allowed the Notejoy product team to develop a hypothesis that their core users wanted intuitiveness and speed, even at the cost of other attributes like customizability.

For an example of how examining negative outliers was helpful, consider a different example at Notejoy.

At some point, Notejoy got a moderate amount of bad feedback on its search feature.

Since people usually wrote in about how much they liked Notejoy’s search, the PM delved into the feedback further. They wanted to understand what users were giving the feedback to see if they could pinpoint the cause for bad feedback.

As it turned out, most of the negative feedback came from users in international markets.

The PM discovered that Notejoy’s search simply doesn’t work as well for non-English alphabets like Chinese or Cyrillic. However, re-engineering search to perform equally well for other languages would be too heavy of a lift for the return.

As a result, the PM developed his hypothesis that their best international product outcomes would come from serving English-speaking countries.

The second tactic to help you develop your hypotheses is context re-creation. If you see users have a consistent issue, try to replicate the problem yourself.

Understanding the actions they’re taking and experiencing how the product behaves relative to their expectation helps you better design an intuitive product for them.

For example, if help search queries are constantly popping up around certain keywords, it’s worth attempting that search in the help center to see what the user sees.

Most commonly, the user is using different terminology than what you’re using.

For example, recall the ‘pin’ vs. ‘save’ Pinterest example. Users thought of ‘saving’ a post as a ‘save’ action, whereas Pinterest called the action ‘pinning.’

This was a problem, because help articles are tagged with specific keywords.

If the user’s searched keywords don’t match the help article’s tagged keywords, then the user won’t be able to solve their problem.

Replicating the user search would allow Pinterest to discover this problem.

After you’ve developed hypotheses on what users want, calibrate your understanding with the feedback system of record.

When you develop a hypothesis that a particular feature should be highly prioritized, look to the FSOR to see if it is as highly requested as you think it is.

If the feature is relatively old, it should rank highly on total requests. Total upvotes represent the absolute number of users who have requested the feature historically.

If the feature is relatively new, it should rank highly on recent requests.

Recent requests represent the total increase in number of upvotes a feature has received in a given time period (often a month).

You want to measure performance on recent requests, because it better indicates relative interest in the feature.

Since the feature hasn’t been around long enough to accumulate a significant amount of requests, it won’t rank highly on total requests.

The time period you choose to measure recent requests over should be:

Long enough that you can see trends instead of noise. There’s lots of day-to-day noise in how many requests any given feature receives. This noise tends to settle out over longer periods of time.

And short enough that you can quickly act on the trend after you see it. This is so you can be more responsive as a product team.

To illustrate this, consider a case where you can identify trending feature requests within a month. Let’s say that you measure trends over a quarter instead.

This means you’ll realize the importance of an emerging feature request 2 months after you could have potentially done so.

A good time period to start with for most companies tends to be a month.

The common finding in this hypothesis calibration step is that the feature is highly requested, but there are a number of other features that are even more highly requested.

To illustrate this, consider an example at Notejoy. One of Notejoy’s competitors, Roam Research, came out with a new feature called bi-directional linking which enabled a novel approach to organizing notes. A significant amount of feedback began surfacing in the feedback river also asking for bi-directional linking support for Notejoy.

Based on this, Notejoy developed a hypothesis that bi-directional linking was a highly requested feature that should be prioritized.

To calibrate that hypothesis though, they first turned to the FSOR.

Since bi-directional linking was a relatively new feature, they measured how it ranked relative to other features on recent requests. It turned out that bi-directional linking was highly requested, but not nearly as much as several other constantly requested features like public notesharing and web clippers.

The Notejoy PM hadn’t realized this, because their meeting chatter tended to orient around the newer, shinier feature requests. Because public notesharing and web clippers were relatively older requests, they registered less prominently in the PM’s mind.

With the FSOR though, the Notejoy PM was able to properly calibrate that public notesharing and web clippers were more important to implement first before tackling bi-directional linking.

The second way you can immediately use these feedback management systems is to conduct rapid feature research.

When you develop a new feature or refine an existing one, you often need to do user research to figure out how to best implement it.

Since feature design requires qualitative detail on what users want, you will primarily use your feedback river for this use case.

The feedback river won’t fully replace your need to do user research, but it will help accelerate how fast you get to key findings.

There are two primary ways the feedback river helps in accelerating your user research:

Initial requirements gathering

User filtering

Initial requirements gathering is getting an initial sense of how to implement your feature.

To do initial requirements gathering, search the feedback river for the feature that you’re thinking of building. This often yields significant initial insight into what users want without even having to set up dedicated interview time with them.

For example, when Notejoy decided to implement tagging, it first searched its Slack feedback river for ‘tagging’-related comments.

It found comments like, “Implement tagging, but do it like Bear (competitor) did it, not like Evernote” and “Bear has best-in-class tagging, please do something similar,” which was tremendously helpful for designing product specs.

Without even investing in user interviews, Notejoy was already able to get direction on how they should design tagging.

Remember that a feature might go by different names, so consider how else your users might have described that feature.

For example, when researching for its ‘web clipper’ feature, Notejoy had to also search for alternate terms like ‘clipper’ or ‘mobile save.’

The second way feedback rivers help you accelerate user research is through user filtering.

A key pain point most PMs face in user research is finding the right users to talk to and getting into a conversation with them.

Most users might not actually have the right context for your research, and will often respond quite slowly to emails asking for time.

Using the feedback river, you can better filter for users who already have an interest in the specific features you are researching and thereby speed up your research.

When searching your feedback river, there are three feedback qualities that indicate you may want to talk with the user: Recent, Detailed, and Polarized.

Recent feedback is judged by the time window of the use case for the feature being requested. If the user feasibly might still be interested in using the feature, they’re recent enough.

Detailed feedback means the user gave very specific feature-implementation ideas.

Polarized feedback means the user has strongly advocated for or criticized the idea.

Each of these feedback qualities raises the probability that the user is:

Emotionally invested in the feature’s implementation or redesign, which makes them more likely to quickly respond to requests to chat.

Knowledgeable on their use case, which makes it easier for you to understand how to best build for them.

And motivated. Motivated users are often willing to give you more interaction than just a one-off user interview, even enabling you to do back-and-forth iteration in some cases. This tremendously improves the efficiency and effectiveness of your user interview process, helping you more quickly complete your user research.

In closing, there are two ways you use your feedback river and system of record: to develop user insight and to conduct rapid feature research.

Developing user insight creates leverage for you by building your product intuition, enabling you to make great product decisions in the absence of concrete information.

Rapid feature research creates leverage for you by improving the efficiency of your user research process.

In the next module, we'll cover lever dashboards, another major source of leverage for great PMs.

Recap: Feedback Management Systems

Great PMs use feedback management systems.

Feedback management systems automate the collection and processing of feedback from multiple sources.

In this module, we discussed the feedback management systems great PMs use to develop and maintain product intuition.

We'll introduce two critical feedback systems: the feedback river and the feedback system of record.

First let's talk about what a feedback river is.

A feedback river is a system that consolidates and integrates your multiple feedback sources into a single channel.

The feedback river provides you with a regularly-updating stream of user data points from multiple feedback sources with which to hone your product intuition.

The channel is usually Slack, or an email mailing list.

The feedback river addresses the first major problem PMs have in developing product intuition: friction in establishing good feedback immersion habits.

First, feedback rivers solve the ‘pull’ relationship with feedback problem by changing the relationship to more of a ‘push’.

As feedback on a certain feature comes in, it automatically gets sent (or “pushed”) to the feedback river, where the PM notices there is fresh feedback to read through.

Second, feedback rivers help reduce friction in accessing feedback.

Most PMs experience friction in accessing feedback from a variety of data sources.

As an example, when a PM wants to access feedback from a new system, the PM has to:

Get access to the system data.

Learn how to pull the system data.

Properly structure a query each time they want to pull the data

and format the query output into a more readable format.

These moments of added friction result in busy PMs looking only at a limited set of feedback sources.

This is a problem, because each feedback source offers a unique perspective on the user.

By looking at a narrow set of feedback sources, PMs miss the most holistic view.

FRs solve this feedback collection friction problem by centralizing all feedback into one easily-accessible channel.

Instead of needing to go into a tool like Google Sheets to dig up NPS survey results and a tool like Asana for customer tickets, data from both automatically feeds into the Slack channel which they are already logged into.

As a result, they only need to skim the feedback river to see the latest data pushes from both sources, and avoid having to log in and query each source.

Additionally, feedback rivers also help PMs develop their product intuition by facilitating a more direct understanding of the user.

FRs solve this secondhand exposure problem by preserving user verbatims from the feedback sources.

But how do you set up your feedback river?

There are overall three key steps: determine feedback river platform, integrate programmatic feedback sources, and integrate manual feedback sources.

We’ve listed these steps for following along in the Reforge Feedback River Template(opens in a new tab).

The first step is to determine your feedback river platform.

The most common platforms used are Slack and an email mailing list, but most internal communication tools work perfectly fine as well.

The most important consideration in choosing between platforms is what platform has the lowest adoption barrier, because regularly looking at the feedback river is the most important part of getting value from it.

The second step is to integrate your programmatic feedback sources into the river.

Programmatic feedback sources are feedback sources for which you can automatically initiate feedback collection and publish to the feedback river.

Some standard feedback sources here might include:

NPS surveys, in-product feedback submissions, and churn surveys.

Outside of these standard sources, there are other feedback sources that give you user insight, but might not be those you typically use. For example: help search queries and social media mentions.

But there are challenges to doing a good integration. The biggest of these challenges for most PMs is handling programmatic inputs at scale.

They start to get too many inputs from certain feedback sources, which leads to other feedback sources not ever being read.

To handle these scaling issues, there are three different types of scaling approaches you can take, depending on the data source:

The first approach you can take is sampling.

Sampling refers to when you publish every nth input instead of every single input.

Selection refers to when you force the input to meet a set of ‘significance criteria’ before being published.

For example, you might require that you get at least 3 or more queries on the same search term before it’s deemed significant enough to push to the feedback river.

This helps you isolate one-off problems from more systematic usability issues users are having.

The third approach you can take is summarization.

Summarization refers to when you rely on a team to distill a large volume of inputs into its core takeaway points.

For example, for NPS, marketing might be in charge of reading through a whole month’s NPS responses and summarizing key takeaways for the feedback river.

The third step is to integrate your manual feedback sources into the river.

Manual feedback sources are feedback sources for which you can’t programmatically collect and publish inputs.

The primary challenge PMs run into in this step is managing stakeholder input.

To solve this problem, designate product champions for each team.

Having a single point of contact for each team makes feedback collection and handling much easier, because you now only deal with one person instead of a whole team.

Once this is done and you’ve integrated your manual feedback sources, you now have a fully running feedback river.

The second feedback management system is the feedback system of record.

The FSOR helps PMs solve the third challenge in developing product intuition: overweighting recent feedback.

Setting up your FSOR is a 3-step process:

Your first step is to determine what platform you want to use for your SOR.

Your second step is to set up your FSOR with the fields you want to track.

Finally, in the most difficult part of the process, set up a regular updating process for the FSOR.

Sachin says “Oftentimes, teams like customer support and sales are really interested in influencing the direction of product. The key is to make sure that they feel heard and that their work is actually shaping the product.”

That concludes setting up your FSOR.

In the module itself, we cover the nuances of using these systems to develop product intuition in more detail.

That closes using feedback management systems and the overall feedback management systems module.

In the next module, we'll cover lever dashboards, another major source of leverage for great PMs.

