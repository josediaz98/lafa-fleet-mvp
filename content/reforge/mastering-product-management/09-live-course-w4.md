# Mastering Product Management Week 4: OKRs as Strategy Communication
> Reforge Live Course | Guest: Michael Ducker (former Head of Product, Mercury; CEO, Valet.ai) | January 17

## Sources

| Source | Type | Speaker | Context |
|--------|------|---------|---------|
| Mastering Product Management W4 | Live Course Session | Instructor + Michael Ducker | Final session of 4-week course on PM leverage |

## Executive Summary

This session represents the capstone of the Mastering Product Management course, bringing together the themes of clarity, focus, and leverage that run through all four weeks. Where previous sessions covered product strategy, customer insights, and roadmaps, this final class addresses OKRs—not as a standalone administrative tool, but as the connective tissue that binds strategy to execution. The central argument is deceptively simple yet widely misunderstood: OKRs are a communication tool for strategy, not a performance management system or a glorified backlog.

Michael Ducker brings a particularly valuable perspective to this discussion. His career arc—from Microsoft intern working on typography features, to Twitter mobile product leader under the guidance of prominent figures, to founder of a credit-building startup acquired by Chime, to establishing the entire product function at Mercury—illustrates the "random walk" that characterizes successful PM careers. More importantly, his experience arriving at Mercury to find 150 people who had just completed their first OKR cycle (which had devolved into eight-hour meetings reviewing product backlogs) provides a concrete case study in organizational transformation. His journey demonstrates that domain expertise is less important than the ability to learn quickly, synthesize complexity for others, and communicate with humility and clarity.

The session's core insight is that most organizations misuse OKRs in one of two ways: treating them as backlogs (lists of work to complete) or as performance evaluation criteria (measuring whether hypotheses came true, rather than whether the PM did good work). Both misuses stem from a fundamental misunderstanding of what OKRs are for. The true purpose is to force clarity around strategy—to make explicit what was implicit, to align teams around shared objectives before work begins rather than jostling for resources at the end, and to create a shared language for discussing whether the organization's bets are paying off.

The practical frameworks offered—particularly the "Reach, Customer Impact, Business Impact" model for structuring key results, and the distinction between health metrics and OKRs—provide concrete tools for implementation. But the deeper lesson is philosophical: OKRs should make your job easier, not harder. If drafting OKRs feels like a massive deadline-driven burden, it's a signal that the underlying work of strategy communication isn't happening continuously. The PM who is constantly discussing strategy with stakeholders, updating narrative documents, and building alignment will find OKR drafting to be a simple documentation exercise. The PM who isn't doing that work will find it painful—and that pain is diagnostic, not inherent to the tool.

Perhaps most valuably, the session addresses when *not* to use OKRs: early-stage startups without product-market fit, teams doing genuinely net-new exploratory work, and infrastructure/reliability teams whose success is measured by maintaining stability rather than driving growth. This nuance prevents the common failure mode of forcing every team into the same framework regardless of their actual work, which breeds cynicism and performative compliance rather than genuine strategic alignment.

---

## 1. The True Purpose of OKRs

### OKRs as Communication, Not Performance Management

The most fundamental insight from this session challenges how most organizations actually use OKRs. Michael Ducker is emphatic: OKRs are a communication tool for strategy, not a mechanism for evaluating employee performance. This distinction matters enormously because confusing the two corrupts both functions.

When organizations use OKR achievement as a performance management input, they create perverse incentives. The entire premise of OKRs—that hitting 70% of an ambitious target represents success—becomes nonsensical when your bonus or promotion depends on that percentage. Rational employees will set conservative targets they can exceed, undermining the tool's purpose of encouraging ambitious bets. Worse, they'll avoid the experimentation and pivoting that good product work requires, because changing direction mid-quarter looks like "not hitting your OKRs."

The proper frame is scientific: OKRs represent hypotheses about what strategies will move the business forward. A scientist whose experiments all confirm their initial hypotheses isn't succeeding—they're not taking enough risk or learning anything new. Similarly, a PM whose OKRs all turn green either set insufficiently ambitious targets or got lucky. The valuable information is in the red and yellow: what did we learn about our assumptions? How should our strategy evolve?

> "Your performance management is not 'did all your hypotheses come true?' That would be ridiculous. Imagine a scientist, if every single experiment was always true—that was success. It doesn't work that way."

### The Mercury Case Study: From Backlogs to Strategy

When Michael arrived at Mercury, the company had just completed its first-ever OKR cycle under the CFO's leadership. The CFO understood that the rapidly scaling company needed measurement and alignment mechanisms, but hadn't grasped the purpose of OKRs as distinct from project management. The result: eight-hour meetings where every team presented four pages of "OKRs" that were actually product backlogs—lists of features to ship, not strategic outcomes to achieve.

The diagnosis matters as much as the cure. Mercury's previous approach treated OKRs as a commitment mechanism: teams would declare what work they planned to do, then be held accountable for completing that work. The fundamental assumption was that if you say you'll do something and then do it, that's success. But this assumption misses the point entirely. Completing planned work matters less than achieving strategic outcomes. A team that ships everything on their backlog but fails to move the metrics that matter has succeeded at the wrong game.

The transformation required education before process change. Michael conducted a "roadshow"—presenting to the executive team first, then to all-hands and team meetings—explaining what OKRs are actually for. This investment in shared understanding was prerequisite to any process change. You cannot simply mandate that people use a tool differently; you must help them understand why the change matters.

> "The first sort of step that I had to do was to understand a little bit, hey, why did you put this system in place? Always don't just jump to a solution. Just learn a little bit, talk to the execs—what was the problem?"

### Why OKR Deadlines Feel So Hard

A revealing moment in the session addresses why PMs so often struggle with OKR deadlines—procrastinating until the last moment, then finding the work painful and the results unsatisfying. The diagnosis is precise: OKRs are a synthesis of strategic work that should be happening continuously. When that underlying work is happening, writing OKRs is easy—just documenting what you've been discussing all along. When it isn't, OKRs become the forcing function for strategy work that should have occurred over weeks, compressed into hours.

The PM who is regularly updating strategy documents, discussing direction with stakeholders, broadcasting intentions and gathering feedback, and having ongoing conversations about priorities will find OKR drafting to be "a quick write down of, oh, this is where we are right now." The PM who hasn't been doing that work will experience OKR season as a crisis—either struggling to articulate strategy for the first time, or discovering that stakeholders fundamentally disagree with their direction.

This reframe is diagnostic, not judgmental. If OKRs feel hard, that's information about gaps in your ongoing communication practices. The solution isn't to get better at last-minute OKR writing; it's to integrate strategy communication into your regular rhythm so that OKRs become documentation rather than creation.

> "OKRs are the communication deliverable at the end—it's not the work. The work is drafting, updating, socializing, talking about your work. And having those conversations with your cross-functional stakeholders."

---

## 2. Crafting Effective Objectives

### Strategy Crystallized in One Sentence

The best objectives are "one-sentence summaries of your strategy that crisply communicate your team's impact on the business, your team's desire of where you're trying to go that's net new expansion, and ideally capture the customer's need somehow in it." This definition packs several requirements that many objectives fail to meet.

First, objectives should be about strategy—the choices you're making about where to invest and what to deprioritize. An objective that could apply to any team isn't strategic. Second, objectives should describe impact on the business, not activity. "Build a recommendation engine" is an activity; "Become the first place customers turn when they need product suggestions" is an impact. Third, objectives should focus on net new expansion—where you're going, not where you've been. This connects to the distinction between OKRs (for growth) and health metrics (for maintenance). Finally, the best objectives are customer-focused rather than company-focused—capturing what changes for the customer, not just what the company wants.

The Google example used throughout the session illustrates this well. An objective like "provide instant search results so users rely exclusively on Google" captures customer experience (instant results), strategic positioning (exclusive reliance), and implicitly the competitive differentiation (faster than alternatives). It's inspirational enough to get people out of bed, specific enough to guide prioritization, and customer-focused rather than internally oriented.

### The Customer-Focused Reframe

One practical technique for improving objectives is to flip business metrics into customer behaviors. David's comment during the session about customer retention illustrates this. "Reduce churn" is technically measurable and strategically relevant, but as Michael notes, it's "about the most boring thing you could probably pick" as an objective. Nobody gets excited about reducing churn.

The reframe: think about what behaviors drive retention. Engaged, loyal users who love your product, share it with others, and return repeatedly—that's the positive state you're pursuing. An objective like "become the first place founders turn when thinking about their banking needs" or "create the business banking experience people recommend to their friends" captures the same strategic intent but frames it positively and customer-centrically.

This isn't just about motivation (though that matters). Customer-focused framing forces clarity about *why* the metric matters. Reducing churn is only valuable because it indicates customer satisfaction and lifetime value. By focusing on what creates satisfaction and value, you're more likely to pursue strategies that actually work rather than short-term metric manipulation.

> "I love flipping things like retention into 'what's the behaviors that drive retention?' It's loving engaged users who want to share your product, talk about your product, use your product more. It's a lot easier to drive a strategy sentence around that positive."

### Objectives Should Last Longer Than a Quarter

A common misconception is that objectives must change every quarter. In practice, good objectives often persist for multiple quarters or even years—as long as you're pursuing that strategy. The Google "instant search" objective might have guided work for years. What changes is the key results: specific measurements of progress that reflect current state, forecasts, and hypotheses.

This distinction matters for team stability and strategic coherence. If objectives change every quarter, teams experience strategy whiplash—never building deep expertise or achieving compounding progress. Stable objectives with evolving key results allow teams to develop mastery while continuously refining their approach based on what they learn.

The signal for changing an objective isn't the quarterly calendar—it's strategic insight. When key results start showing that further progress on an objective no longer meaningfully impacts the business, or when external conditions fundamentally change what matters, that's when objectives should evolve. But this should be a deliberate strategic choice, not a default assumption that everything resets every 12 weeks.

> "Objectives lasting multiple quarters is super fine, super normal. Key results should definitely be updated to reflect the current measurement and your forecast and predictions. But the objective that it aligns to—100%, that's normal, and it helps create stable teams to actually have stable goals."

---

## 3. Building Measurable Key Results

### The Reach-Impact-Business Framework

Michael offers a practical framework for structuring key results that captures the "overall shape of the strategy." Rather than picking arbitrary metrics, this framework ensures you're measuring different dimensions of success:

1. **Reach**: How many humans were impacted or performed the behavior you're targeting? This validates that your strategy is actually touching enough people to matter. A brilliant feature that nobody uses isn't strategic success.

2. **Customer Impact**: How well did you serve the customers you reached? This measures quality, not just quantity. Speed, satisfaction, task completion, error rates—metrics that capture whether customers got value from their interaction.

3. **Business Impact**: Does this drive outcomes that matter for the business? Revenue, retention, expansion, efficiency—the ultimate "so what" of your work.

Using the Google search example: reach might be daily active searchers, customer impact might be search result latency (250ms → 50ms), and business impact might be search volume growth or advertising revenue. Each measures something distinct; together they paint a complete picture.

This framework isn't rigid—you can have two or four key results instead of three, and the specific metrics depend on your context. But the underlying logic is valuable: measure that you're reaching enough people, that you're serving them well, and that it matters for the business. Missing any dimension creates blind spots.

> "I love key results measured in the quantity and the quality of what you did. We often go in threes—reach of my strategy, impact on the customer, impact on your business."

### Outcomes Over Outputs

The session draws a sharp distinction between output-oriented and outcome-oriented key results. Output-oriented KRs measure what you shipped: "Launch feature X by July 31," "Complete database migration," "Ship mobile app v2.0." Outcome-oriented KRs measure what changed as a result: "10,000 customers using new feature," "Database query latency under 100ms," "Mobile conversion rate increased 15%."

The problem with output-oriented KRs isn't that outputs don't matter—shipping things is real work. The problem is that outputs don't answer "so what?" Shipping a feature that nobody uses, or that doesn't improve the metrics that matter, isn't strategic success. It's motion without progress. Output orientation also subtly discourages the pivots and iterations that good product development requires. If your KR is "launch by July 31" and you discover in June that the feature needs a fundamental redesign, the KR creates pressure to ship something—anything—rather than to achieve the actual goal.

Outcome orientation keeps focus on what matters. If your KR is "10,000 customers successfully completing onboarding in under one hour," you're free to pursue whatever approach achieves that. Maybe it's the feature you originally planned; maybe it's something simpler; maybe it's process changes rather than software. The flexibility to find the best path is preserved.

One practical technique: when you have what feels like an output-oriented KR ("launch feature by date"), ask what outcome that output is meant to achieve, then measure the outcome. "Launch new onboarding flow" becomes "at least 1,000 customers have completed new onboarding flow with average time under 15 minutes." This preserves accountability while allowing flexibility in execution.

> "Outcome-oriented measurement—you're getting much more specific with 'this is the thing that we're changing that matters to the business.' This is the lever we've identified that's going to accelerate the business. Outcomes answer the why."

### Shared Objectives, Separate Key Results

A nuanced challenge arises when multiple teams contribute to the same strategic objective. Mercury's onboarding speed example illustrates this: both a software team (building automation) and an operations team (manual review processes) could improve onboarding time. The temptation is to give both teams the same KR: "reduce onboarding time from one day to one hour."

Michael argues against shared KRs, not against shared objectives. The problem with shared KRs is accountability and learning: if onboarding time improves, which team's work drove it? If it doesn't improve, who should do something different? Shared metrics obscure individual contribution and prevent the organization from learning which investments have the highest leverage.

The solution: share the objective ("fastest business bank onboarding in the industry") but have each team own distinct KRs that measure their specific contribution. The software team might measure automation rate and AI-assisted review accuracy. The operations team might measure manual review throughput and first-pass approval rate. Both contribute to the shared objective, but each is accountable for their specific lever.

This approach preserves collaboration—teams genuinely working toward the same goal—while maintaining clarity about who's responsible for what. It also enables strategic learning: if the software investments dramatically outperform the operations investments (or vice versa), that informs future resource allocation.

> "If you're sharing a key result, you probably should just be on the same team. I love sharing objectives—sharing objectives is about sharing strategy. But key results, I want to help teams better articulate their contribution to that shared objective."

---

## 4. OKRs vs. Health Metrics

### The Fundamental Distinction

Perhaps the session's most practically useful insight is the distinction between OKRs and health metrics—and when to use each. The principle: OKRs are for strategic, net-new work that expands the business. Health metrics are for maintenance, reliability, and sustaining what you've already built.

Think of it through a racing metaphor: once you've completed a lap of the track, you don't want to keep re-measuring that past lap. It becomes part of the momentum you've built—something to maintain, not to strategize about. Health metrics measure the laps you've already completed: uptime, baseline performance, existing feature usage, current customer satisfaction. OKRs measure the new laps you're trying to complete: expanding to new customer segments, improving key experiences, adding new capabilities.

This distinction has profound implications for how teams are evaluated. A reliability team maintaining 99.99% uptime is doing critical work, but it's not strategic in the OKR sense—there's no "net new expansion of the business" happening. Forcing such teams into OKR frameworks creates frustration and performative compliance. They should absolutely measure their work and be recognized for it, just through health metrics rather than OKRs.

> "OKRs should not cover all your work. The strategy of how to build new stuff is really tied to the expansion of your business, the expansion of your TAM, customers, service areas, or quality improvements. That's all very measurable stuff. It's not, in my opinion, well tied to things like health metrics."

### When Health Metrics Become OKRs

The boundary between health metrics and OKRs isn't fixed—it shifts based on strategic necessity. The key question: is maintaining or improving this metric part of your strategy, or is it table stakes?

If a metric represents something you've already achieved and just need to sustain, it's a health metric. If you're at 99.99% uptime and your customers are happy with that, uptime is a health metric—monitor it, maintain it, don't make it an OKR. But if you're experiencing reliability problems that are causing customer churn and eroding trust, reliability becomes strategic. Improving reliability is no longer maintenance; it's expansion (expanding customer trust, expanding retention). At that point, it makes sense as an OKR.

The same applies to any metric. Customer satisfaction might be a health metric if you're at target levels and stability is the goal. But if satisfaction has dropped and recovering it is critical to business strategy, it becomes an OKR. The distinction isn't about the metric itself but about whether improving it is part of your strategic expansion or part of maintaining baseline operations.

> "If you're dealing with a lot of reliability problems, downtime, products failing, customers losing trust—your health metric is no longer a health metric. It's an OKR. It's something that you have to have a strategy to build trust with your customers."

### Recognizing Maintenance Work

A practical challenge: how do you ensure that teams doing health metric work feel valued and recognized? The temptation in OKR-centric organizations is to push everyone into the same framework, which either forces maintenance teams to create artificial OKRs or leaves them feeling like second-class citizens.

Michael's approach at Mercury had two components. First, he allowed maintenance-focused teams to have minimal OKRs—perhaps one objective with three key results, compared to growth teams with three objectives and nine key results. This acknowledged that their strategic work was a smaller portion of their total work, without forcing them out of the system entirely.

Second, and more importantly, he ensured that health metrics got equal airtime in company communications. The same forums where OKR progress was discussed—all-hands meetings, product reviews—included prominent discussion of health metrics. Revenue, uptime, baseline customer satisfaction—these got visibility and celebration alongside strategic wins. The goal was "the conversation to be equal even if the process or the tool we used shifted."

The psychology matters: teams doing maintenance work need to know their contribution is valued, even if it's measured differently. The metric types differ, but the organizational recognition shouldn't.

> "I gave a space and encouraged the health metrics to have equal weighting. Because it's important that we talk about how we're making money. Revenue is a health metric. We are staying up and serving our customers. That's a health metric. I want to make sure that our teams are getting rewarded for the fact that it still happened."

---

## 5. Cross-Functional Alignment

### Moving Prioritization to the Beginning

One of the most significant changes Michael drove at Mercury was shifting when prioritization decisions occurred. In the previous system, teams committed to backlogs at the beginning of a cycle, then fought for resources at the end as deadlines approached. The implicit message: everything is equally important until the last minute, when reality forces hard choices.

The OKR transformation inverted this. By establishing company-wide objectives at the start of the period, teams knew what was strategically important before they began planning their work. This moved the hard prioritization conversations from the end (when it's too late to adjust course meaningfully) to the beginning (when teams can actually plan around priorities).

This shift required confronting a cultural challenge: at a high-growth, cash-rich company, the temptation was to do everything. Why prioritize when you can hire more people? But unlimited expansion creates its own problems—coordination costs, strategic incoherence, diffusion of focus. Forcing early prioritization meant some teams heard "your thing isn't top priority this quarter," which was uncomfortable but necessary for organizational alignment.

> "This process of actually bringing that first stab at objectives top down actually moved that prioritization discussion from being at the end of the whole development process to the beginning. That really forcing prioritization earlier was a big shift in thinking about product development."

### The Solicitation-Synthesis Process

Mercury's OKR process combined bottom-up input with top-down synthesis. First, teams were asked to suggest candidate objectives from their domain expertise—what do you think should be company objectives based on what you're seeing? This solicitation ensured that leaders didn't miss important signals from the front lines.

Then the leadership team (CEO, CTO, CFO, Head of Product, marketing leadership) synthesized these inputs into the company's key objectives. This wasn't voting or averaging—it was leadership judgment informed by team input. The resulting objectives represented executive alignment on what mattered most.

Finally, teams were asked to align their work to these company objectives first, before proposing additional objectives or health metrics. This created a constraint: your top priority should be contributing to company priorities. Only after that alignment was established could teams surface other important work.

This process balanced autonomy with alignment. Teams had input into priority-setting and autonomy in how they achieved objectives. But they didn't have unlimited freedom to pursue whatever seemed locally optimal. The constraint of company objectives ensured organizational coherence.

> "We solicited objectives from the team—just give us some candidate objectives you care about from your domain expertise. An executive group then sat down and said, okay, these are the objectives of the company. And the request to the teams was: align your work first to these objectives."

### Strategy Squads for Cross-Functional Work

When multiple teams share an objective but have separate KRs, coordination becomes critical. The instructor suggests the concept of "strategy squads"—regular cross-functional meetings where everyone pursuing a shared objective checks in on progress, shares learnings, and identifies ways to help each other.

The cadence might be biweekly: representatives from each team working on the objective gather to discuss how things are going. Are our separate efforts producing the expected results? Is anyone blocked where another team could help? Have we learned anything that changes our approach? This isn't OKR governance—it's collaborative problem-solving among people with aligned goals.

Such squads serve multiple purposes: coordination (ensuring efforts don't conflict), learning (sharing what's working), motivation (seeing that you're part of something larger), and course correction (identifying when the shared objective needs revision). They institutionalize the cross-functional communication that makes shared objectives actually work.

> "I love the idea of a strategy squad—you've identified the strategy, actually bringing people together around the strategy. You share this objective. You're all committed to it. All bring your own OKRs you can talk about. And that just feels like a really good team collaboration cross-function that you'd run every two weeks."

---

## 6. When NOT to Use OKRs

### Early-Stage Startups and Net-New Work

OKRs are poorly suited for situations where you don't yet know what to measure. In true early-stage startups without product-market fit, or in net-new product lines within larger companies, the entire premise of OKRs breaks down. You can't set meaningful key results when you're still discovering what success looks like.

The work at this stage is fundamentally different: building things to get customer feedback, iterating rapidly, pivoting when hypotheses are wrong. You might measure your responsiveness (how quickly do you ship in response to customer feedback?) or your learning velocity (how many hypotheses have you tested?), but these meta-metrics aren't the same as strategic OKRs. They're measuring the process of discovery, not progress toward a known destination.

For teams in this phase, the right tools are different: clear problem statements, customer feedback loops, rapid experimentation frameworks. Forcing OKRs onto genuinely exploratory work creates artificial constraints that impede the flexibility needed for discovery.

> "When you're doing true net new work—an actual startup, no product market fit, nothing to optimize for—the OKR focus gets really difficult because you actually don't know what to measure yet. You're doing a bunch of work to see if customers respond."

### Reliability and Infrastructure Teams

Teams focused on maintaining existing systems face a different challenge: their success is often defined by *nothing happening*. No outages, no regressions, no security incidents. This is critically important work, but it doesn't fit the OKR framing of "net new expansion of the business."

Michael is explicit: managers shouldn't force OKRs on such teams just because their PM peers are using them. Reliability teams should "have your own process, your own thing to be proud of, your own cadence to talk about it, your own slide in the all-hands." The tool is different, but the recognition and rigor should be equivalent.

When reliability teams try to shoehorn their work into OKRs, the results are typically either trivially easy to achieve (maintain current uptime) or artificially ambitious (achieve unprecedented uptime levels that may not actually matter to customers). Neither serves the actual purpose of aligning strategy and measuring progress.

> "If you're a team really focused on sustainability, reliability, health metrics—I've met many people who've been pressured to do OKRs because the rest of their PM peers are doing it. I have the controversial opinion that's not in this curriculum: your manager should not be telling you to do OKRs, and you shouldn't be doing OKRs."

### The Shoehorning Problem

A common failure mode: organizations mandate that all teams use OKRs, regardless of whether the tool fits their work. This creates performative compliance—teams going through the motions of OKR processes without gaining the strategic alignment benefits. Worse, it breeds cynicism: people learn that OKRs are bureaucratic theater rather than useful tools.

The session's perspective is that OKRs should be used selectively, for the types of work they're designed to support. A portfolio approach makes sense: strategic growth work uses OKRs, maintenance work uses health metrics, exploratory work uses discovery frameworks. The goal is choosing the right tool for each situation, not forcing uniformity.

This requires leadership judgment and, sometimes, courage. Saying "you don't need OKRs" to a team can feel like saying "you're not important." The framing matters: it's not that their work doesn't matter, it's that their work is measured differently. Both growth and maintenance are essential; they're just different types of contribution.

> "OKRs are the tool for measuring iterative changes that grow the TAM and aligning the rest of the company around that. The net new areas are about the long-form storytelling and the actual product shipping of finding where the unique need is in the market. So I'm a big fan of not doing OKRs there."

---

## 7. The OKR Lifecycle

### Continuous Process, Not Quarterly Event

The session emphasizes that effective OKR practice is continuous, not punctuated by quarterly deadlines. The lifecycle diagram discussed shows an ongoing cycle: drafting, updating, socializing, launching, reinforcing, reviewing—then back to drafting. At any given moment, a team is somewhere in this cycle, and the work never fully stops.

This continuous framing has practical implications. OKR drafting shouldn't feel like a major project because the underlying strategy work should be happening constantly. Socialization isn't a one-time launch but ongoing communication as understanding deepens. Review isn't a post-mortem but an ongoing assessment of whether hypotheses are playing out.

The quarterly rhythm exists for coordination—syncing up across teams, making company-wide priority adjustments, formally documenting where things stand. But treating the quarterly deadline as the main event misunderstands the tool. The main event is the daily and weekly work of strategic communication.

> "This job never ends. This whole cycle never ends. And as a PM, or if you get to the point in your career where you're managing PMs and multiple teams, you're actually thinking about this pretty much nonstop."

### Leveraging Existing Processes

One of the most practical recommendations: don't create new OKR-specific meetings and rituals. Instead, integrate OKR discussion into processes that already exist. Sprint reviews can start with a reminder of current objectives and progress toward key results. All-hands presentations can include OKR updates alongside other business metrics. One-on-ones can reference OKRs as context for discussing work priorities.

This approach has multiple benefits. It reduces meeting overhead (no separate "OKR review meeting"). It normalizes OKR discussion as part of regular work rather than a separate bureaucratic layer. It creates natural opportunities for reinforcement without requiring everyone to remember yet another recurring meeting.

The key is designing templates and agendas that bake in OKR reference. If the sprint review slide template includes "current team objective" and "KR progress," people will fill it in. If it doesn't, they'll forget.

> "Use the tools that you already have. I'd always love having OKRs as the first slide template for sprint review: what are your OKRs for the team? What are you actually trying to achieve? And then prioritized epics—well, why are you working on epic number three when you should be working on epic number one?"

### Strategy Documents as Foundation

Michael offers a valuable complement to OKR practice: maintaining longer-form strategy documents that provide context for the condensed OKR format. OKRs are synthesis—shorthand references to strategic thinking. But the underlying strategic thinking needs a home, and that home is the strategy document.

The strategy document might be 500 words or several pages. It explains the rationale behind objectives, the hypotheses embedded in key results, the alternatives considered and rejected. When questions arise about why an objective was chosen or what a key result really means, the strategy document provides answers.

This relationship between strategy documents and OKRs has a practical benefit: it takes pressure off OKRs to carry explanatory weight they're not designed for. OKRs can be crisp and condensed because the full context lives elsewhere. People who need more detail know where to find it.

> "OKRs are a synthesis of your strategy—a shorthand way of communicating to other teams. I don't find they're great at the long-form discussion. Create space to have a conversation about strategy in a strategy document. Keep writing 500-word memos. Have that ongoing discussion. Then the OKRs are pulled from that longer-form work instead of being the vehicle of communication."

---

## Connections Map

The themes in this session interlock to form a coherent philosophy of OKR practice that differs significantly from how most organizations actually use the tool.

**Communication as Core Purpose**: Everything flows from understanding that OKRs are primarily a communication tool. This insight resolves many common OKR frustrations. If OKRs are for communication, then the pain of deadline-driven drafting reveals insufficient ongoing communication (Theme 7). If they're for communication, then forcing them onto maintenance teams that don't need strategic communication makes no sense (Theme 6). If they're for communication, then the distinction from backlogs becomes clear—backlogs communicate work plans, OKRs communicate strategy (Theme 1).

**The Objective-KR Relationship**: Themes 2 and 3 describe a specific relationship between objectives and key results that differs from common practice. Objectives crystallize strategy and focus on customer impact; they can persist across multiple quarters. Key results measure progress toward objectives and should be updated quarterly based on learning. This framing prevents objectives from becoming either too vague (mere aspirations) or too specific (output targets).

**The OKR Boundary**: Themes 4 and 6 work together to define where OKRs apply. OKRs are for growth work—strategic expansion of the business. Health metrics are for maintenance work—sustaining what's already built. Neither is more valuable; they're simply measured differently. Teams should use the tool appropriate to their work, not force everything into one framework.

**Cross-Functional Coordination**: Theme 5 addresses how OKRs enable alignment across teams. Shared objectives create shared language and direction; separate KRs maintain accountability. The process of soliciting input, synthesizing at leadership level, and communicating back creates genuine alignment rather than top-down imposition.

**Continuous Practice**: Theme 7 synthesizes everything into a way of working. OKR practice isn't a quarterly event but a continuous cycle supported by strategy documents, integrated into existing processes, and treated as fundamental to how PM work happens. When done this way, OKRs become invisible infrastructure rather than burdensome overhead.

The meta-insight: OKRs are easy when everything else is working. If strategy is clear, if communication is frequent, if alignment is genuine, if the right teams are using the right tools—then OKRs document what already exists. If any of those elements are missing, OKRs become a painful forcing function that reveals the gaps. The gaps, not the OKRs, are the real problem.

---

## Action Items

### Immediate (This Week)

1. **Audit your current OKRs**: Are they actually objectives and key results, or are they backlogs? Apply the outcome vs. output test—do your KRs measure what changed, or what you shipped?

2. **Check team alignment**: Ask your team members what they think your objective is. Don't expect identical words, but look for conceptual alignment. Misalignment reveals communication gaps.

3. **Identify health metrics vs. OKRs**: Review your team's metrics. Which represent strategic growth to pursue? Which represent baselines to maintain? Label them appropriately.

### Short-term (This Quarter)

4. **Integrate OKRs into existing processes**: Modify sprint review templates, all-hands presentations, or regular team meetings to include OKR reference. Make strategic context part of routine work, not a separate bureaucracy.

5. **Create or update your strategy document**: Write the longer-form rationale behind your OKRs. This becomes the reference when questions arise and takes pressure off OKRs to carry explanatory weight.

6. **Apply the Reach-Impact-Business framework**: For each objective, ensure you have key results measuring reach (how many affected), customer impact (how well served), and business impact (what it means for the company).

### Strategic (Ongoing)

7. **Decouple OKRs from performance management**: If your organization ties OKR achievement to compensation or evaluation, advocate for change. Explain the perverse incentives this creates and the scientific framing of OKRs as hypotheses.

8. **Establish cross-functional strategy squads**: For objectives shared across teams, create regular touchpoints where contributing teams coordinate, share learnings, and help each other.

9. **Communicate continuously**: If OKR drafting feels hard, it's a signal to increase ongoing strategy communication. Build habits of regular updates, stakeholder discussions, and alignment checks so that OKRs become documentation rather than creation.

---

## Critical Gaps & Limitations

**Single Company Depth**: While Mercury provides a detailed case study, the session lacks contrasting examples from different company types, stages, or industries. Mercury's specific situation (high-growth fintech, first product leader, CFO-initiated process) may not generalize.

**Tooling Ambiguity**: The session doesn't address OKR tooling—software for tracking, templates for documentation, integration with other systems. Practical implementation requires these details.

**Cascading Complexity**: The discussion of how OKRs cascade from company to team level remains somewhat abstract. The critique of "parent's KR becomes child's O" pattern is clear, but the recommended alternative could use more concrete examples.

**Cultural Prerequisites**: The session assumes a certain level of organizational maturity—leadership willing to engage in strategy discussions, teams capable of distinguishing strategy from execution, culture that tolerates ambitious failure. Organizations lacking these prerequisites may struggle to implement the recommended approach.

**Quantification Challenges**: Some valuable objectives resist quantification. How do you measure "become the trusted partner for startup banking"? The session acknowledges outcomes over outputs but doesn't fully address genuinely hard-to-measure strategic directions.

---

## Appendix: Key Quotes

| Topic | Quote | Speaker |
|-------|-------|---------|
| OKR Purpose | "OKRs are a communication tool for strategy—not a performance management tool, not a backlog." | Michael Ducker |
| Performance Disconnect | "Your performance management is not 'did all your hypotheses come true?' That would be ridiculous. Imagine a scientist, if every single experiment was always true—that was success." | Michael Ducker |
| Deadline Difficulty | "OKRs are the communication deliverable at the end—it's not the work. The work is drafting, updating, socializing, talking about your work." | Michael Ducker |
| Good Objectives | "The best objectives are one-sentence summaries of your strategy that crisply communicate your team's impact on the business and ideally capture the customer's need somehow in it." | Michael Ducker |
| Customer Focus | "I love flipping things like retention into 'what's the behaviors that drive retention?' It's a lot easier to drive a strategy sentence around that positive." | Michael Ducker |
| Objective Duration | "Objectives lasting multiple quarters is super fine, super normal. Key results should definitely be updated. But the objective—100%, that's normal, and it helps create stable teams." | Michael Ducker |
| KR Framework | "I love key results measured in the quantity and the quality of what you did. We often go in threes—reach of my strategy, impact on the customer, impact on your business." | Michael Ducker |
| Outcomes vs Outputs | "Outcome-oriented measurement—you're getting much more specific with 'this is the thing that we're changing that matters to the business.' Outcomes answer the why." | Michael Ducker |
| Shared KRs | "If you're sharing a key result, you probably should just be on the same team. I love sharing objectives—sharing objectives is about sharing strategy." | Michael Ducker |
| Health Metrics | "OKRs should not cover all your work. The strategy of how to build new stuff is really tied to the expansion of your business. It's not well tied to things like health metrics." | Michael Ducker |
| When Health → OKR | "If you're dealing with reliability problems, downtime, customers losing trust—your health metric is no longer a health metric. It's an OKR." | Michael Ducker |
| Equal Recognition | "I gave space and encouraged health metrics to have equal weighting. Revenue is a health metric. Staying up is a health metric. Teams should get rewarded for that." | Michael Ducker |
| Prioritization Timing | "This process moved that prioritization discussion from being at the end of the development process to the beginning. Forcing prioritization earlier was a big shift." | Michael Ducker |
| Bottom-Up + Top-Down | "We solicited objectives from the team—candidate objectives from your domain expertise. Then the executive group said, these are the company objectives. Align your work first to these." | Michael Ducker |
| Strategy Squads | "I love the idea of a strategy squad—bringing people together around the strategy. You share this objective, all committed to it, all bring your own OKRs." | Instructor |
| When Not to OKR | "When you're doing true net new work—no product market fit, nothing to optimize—the OKR focus gets really difficult because you don't know what to measure yet." | Michael Ducker |
| Maintenance Teams | "Your manager should not be telling you to do OKRs, and you shouldn't be doing OKRs. You should have your own process, your own thing to be proud of." | Michael Ducker |
| Continuous Process | "This job never ends. This whole cycle never ends. As a PM or managing PMs, you're thinking about this pretty much nonstop." | Instructor |
| Strategy Documents | "OKRs are a synthesis of your strategy—a shorthand way of communicating. Create space to have conversation about strategy in a strategy document. Write 500-word memos." | Michael Ducker |
| PM Leverage | "The job of the PM is to provide clarity and focus. These are tools to help you make better decisions to serve your end customer." | Instructor |
| Career Flexibility | "Embrace the random walk. It'll actually strengthen the muscle to see different things. You're not going to get shoehorned—try different problems." | Instructor |
| Learning and Teaching | "One of the magical things of being a PM is that leverage—it's not about your age or career experience. It's often about holding the mic. Taking in data, broadcasting back data." | Michael Ducker |

---

## Source Reference

**Mastering Product Management Week 4: OKRs**
- Platform: Reforge Live Course
- Date: January 17
- Duration: ~90 minutes
- Instructor: Reforge course facilitator (unnamed)
- Guest: Michael Ducker
  - Former Head of Product, Mercury
  - CEO & Co-founder, Valet.ai
  - Previous: Twitter Mobile Product, Pinch (credit-building startup acquired by Chime), Microsoft
  - LinkedIn: Available for connection
  - Company: valet.ai
